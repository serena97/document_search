{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/miniconda3/envs/search/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.marco import get_sentences\n",
    "from model.word2vec import get_model, train_model\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "raw_sentences = get_sentences()\n",
    "tokenized_sentences = [simple_preprocess(sentence) for sentence in raw_sentences]\n",
    "skipgram_model = get_model(tokenized_sentences)\n",
    "train_model(skipgram_model, tokenized_sentences)\n",
    "\n",
    "model_path = \"./artifacts/word2vec-300.bin\"\n",
    "skipgram_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10019273, -0.33374676, -0.02642345,  0.41534606,  0.14714254,\n",
       "       -0.06856732,  0.43219516, -0.10808956,  0.19691965,  0.07631785,\n",
       "        0.10408203,  0.07161479,  0.07158351, -0.17790368,  0.21030354,\n",
       "        0.13951635,  0.36059147, -0.30308104,  0.10031758, -0.07560526,\n",
       "       -0.0139124 , -0.12335718,  0.31065795, -0.09804939,  0.10784882,\n",
       "        0.14239235, -0.0718799 , -0.18125203,  0.07639895, -0.10468737,\n",
       "        0.20515351,  0.19570537, -0.24486354,  0.02445136,  0.00948732,\n",
       "       -0.13417399,  0.21692474,  0.08664848,  0.20024656,  0.06315239,\n",
       "        0.00064163,  0.02665208,  0.01468457, -0.25618294, -0.24128504,\n",
       "        0.13493638,  0.0933916 ,  0.16827904, -0.20723066, -0.2389881 ,\n",
       "        0.08144969,  0.01782364,  0.18259524,  0.11810031, -0.30440283,\n",
       "        0.16046287,  0.04261621,  0.04558365,  0.22207616,  0.30472726,\n",
       "       -0.14382133, -0.281999  , -0.01817146, -0.33375046,  0.13382258,\n",
       "       -0.27748376, -0.06398495, -0.21484396,  0.11975351, -0.3418576 ,\n",
       "       -0.2038699 , -0.2090184 ,  0.11047591, -0.07018534, -0.14731587,\n",
       "        0.01930094, -0.1119951 ,  0.06891933, -0.00597616,  0.07096303,\n",
       "       -0.23357412, -0.09526274,  0.01686052, -0.23074815,  0.41047677,\n",
       "       -0.19032776,  0.24719092, -0.15708584, -0.06355345, -0.11181126,\n",
       "       -0.01486752, -0.2903674 , -0.1487386 , -0.22600749,  0.37418112,\n",
       "       -0.23776035,  0.01557568, -0.11444309,  0.04309598,  0.30190605,\n",
       "       -0.00562633,  0.16458869, -0.05647381, -0.00500806,  0.09546142,\n",
       "       -0.14423387, -0.43100145, -0.42507607, -0.12008292, -0.12048333,\n",
       "        0.07319006, -0.51332164, -0.34846878,  0.07937451, -0.15623525,\n",
       "       -0.23189147, -0.04669496, -0.1836094 ,  0.09179436,  0.30469978,\n",
       "        0.29468942,  0.44885308,  0.23982887,  0.13716829, -0.09651715,\n",
       "        0.02246823,  0.3072571 ,  0.28406623,  0.03166452,  0.2151227 ,\n",
       "       -0.535484  , -0.11822701, -0.07384676, -0.04684233,  0.19502214,\n",
       "        0.07212427, -0.0968105 , -0.32045826,  0.02235031,  0.11884041,\n",
       "       -0.25492412, -0.27884495, -0.0351987 ,  0.13319086,  0.09550943,\n",
       "        0.02972701, -0.1593238 , -0.08088677,  0.00761338,  0.16988626,\n",
       "       -0.00602417,  0.14576182, -0.39621788, -0.25837952,  0.24248081,\n",
       "        0.16832866, -0.35487857, -0.18527177, -0.16567767, -0.10647431,\n",
       "        0.1325382 , -0.01513396, -0.18946862,  0.26078403, -0.18224554,\n",
       "       -0.13390756,  0.05248442,  0.08927368,  0.10092818,  0.06181649,\n",
       "        0.33214724,  0.42092738,  0.46317145, -0.06087366, -0.10512024,\n",
       "        0.07316776, -0.34866315,  0.20203054, -0.3181216 , -0.17939599,\n",
       "        0.23033932, -0.00110882, -0.18049225, -0.31487808, -0.29186675,\n",
       "        0.29941154, -0.2081414 ,  0.11184438, -0.08189943, -0.0789007 ,\n",
       "        0.08277162,  0.08672202, -0.24627125,  0.02526221,  0.07429026,\n",
       "        0.06727695, -0.00233518, -0.28790358, -0.16516916, -0.23066735,\n",
       "        0.10644489, -0.34776494,  0.10211436,  0.21724588, -0.03469886,\n",
       "        0.14651279,  0.00090471,  0.17500506,  0.01058656, -0.19606867,\n",
       "        0.1672234 , -0.11732192,  0.01379063, -0.28584862, -0.07214277,\n",
       "       -0.25932822, -0.2096046 , -0.16333592,  0.06129013, -0.30140087,\n",
       "        0.01886799, -0.29086202,  0.37004226,  0.20348313,  0.10584749,\n",
       "        0.01474092,  0.01939763,  0.35216925,  0.09072611, -0.00567453,\n",
       "        0.33527416, -0.12308047, -0.08353101,  0.00375574,  0.4303705 ,\n",
       "       -0.0125265 , -0.32820326, -0.09336863, -0.20748755,  0.05387058,\n",
       "       -0.06120495,  0.12005544,  0.13921452, -0.39192995,  0.10083999,\n",
       "        0.09445489,  0.09882611, -0.12622873, -0.25139785, -0.02593241,\n",
       "       -0.21625853, -0.10504302, -0.07705908, -0.1405221 , -0.08640997,\n",
       "        0.02235823,  0.34543043,  0.2646003 , -0.10268451, -0.40500847,\n",
       "        0.12402137, -0.0795847 ,  0.21313743,  0.3165099 , -0.13593958,\n",
       "        0.00236033,  0.27644202, -0.04485533,  0.18358442,  0.0263349 ,\n",
       "       -0.17587157, -0.26086107,  0.05039261,  0.16950943,  0.14660758,\n",
       "       -0.17673022,  0.15389815, -0.02736748, -0.09946994, -0.27839416,\n",
       "        0.32171366, -0.00499423, -0.13406466,  0.00112274, -0.0858909 ,\n",
       "        0.04904288, -0.21315554, -0.10192718,  0.12460513,  0.24657525,\n",
       "        0.08701424, -0.32530704,  0.05716846, -0.15754741, -0.01188467,\n",
       "        0.01255137,  0.1230954 , -0.47484925, -0.01075347,  0.1356196 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_path = \"./artifacts/word2vec-300.bin\"\n",
    "model = Word2Vec.load(model_path)\n",
    "model.wv['what']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Unzip the batch into lists of queries, relevant_docs, and irrelevant_docs\n",
    "    query_emb_list, relevant_doc_emb_list, irrelevant_doc_emb_list = zip(*batch)\n",
    "    \n",
    "    # Pad sequences for relevant and irrelevant documents\n",
    "    # This assumes each document is already a tensor of embeddings; adjust if the structure is different\n",
    "    padded_relevant = pad_sequence([pad_sequence(docs, batch_first=True) for docs in relevant_doc_emb_list], batch_first=True)\n",
    "    padded_irrelevant = pad_sequence([pad_sequence(docs, batch_first=True) for docs in irrelevant_doc_emb_list], batch_first=True)\n",
    "    \n",
    "    # Pad the sequences of query embeddings\n",
    "    query_emb_tensor = pad_sequence(query_emb_list, batch_first=True)\n",
    "\n",
    "    return query_emb_tensor, padded_relevant, padded_irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "starting training\n",
      "Triplet loss: 0.30319374799728394\n",
      "Triplet loss: 0.29807257652282715\n",
      "Triplet loss: 0.29265469312667847\n",
      "Triplet loss: 0.3056081235408783\n",
      "Triplet loss: 0.2967092990875244\n",
      "Triplet loss: 0.3004317283630371\n",
      "Triplet loss: 0.29340994358062744\n",
      "Triplet loss: 0.30496418476104736\n",
      "Triplet loss: 0.3018951714038849\n",
      "Triplet loss: 0.3007829487323761\n",
      "Triplet loss: 0.2992069125175476\n",
      "Triplet loss: 0.2943861484527588\n",
      "Triplet loss: 0.2912207245826721\n",
      "Triplet loss: 0.29734379053115845\n",
      "Triplet loss: 0.29820194840431213\n",
      "Triplet loss: 0.29773378372192383\n",
      "Triplet loss: 0.3020175099372864\n",
      "Triplet loss: 0.28082746267318726\n",
      "Triplet loss: 0.3164953291416168\n",
      "Triplet loss: 0.3053825795650482\n",
      "Triplet loss: 0.28426867723464966\n",
      "Triplet loss: 0.3031502962112427\n",
      "Triplet loss: 0.3004280924797058\n",
      "Triplet loss: 0.3054913282394409\n",
      "Triplet loss: 0.29094356298446655\n",
      "Triplet loss: 0.2966930866241455\n",
      "Triplet loss: 0.29426509141921997\n",
      "Triplet loss: 0.29119622707366943\n",
      "Triplet loss: 0.30320054292678833\n",
      "Triplet loss: 0.29180586338043213\n",
      "Triplet loss: 0.2962495684623718\n",
      "Triplet loss: 0.30404865741729736\n",
      "Triplet loss: 0.3086671829223633\n",
      "Triplet loss: 0.28506070375442505\n",
      "Triplet loss: 0.2673746645450592\n",
      "Triplet loss: 0.18680492043495178\n",
      "Triplet loss: 0.2192331999540329\n",
      "Triplet loss: 0.13849617540836334\n",
      "Triplet loss: 0.12025873363018036\n",
      "Triplet loss: 0.36167004704475403\n",
      "Triplet loss: 0.18073393404483795\n",
      "Triplet loss: 0.2724306583404541\n",
      "Triplet loss: 0.16491085290908813\n",
      "Triplet loss: 0.12401182949542999\n",
      "Triplet loss: 0.09630703926086426\n",
      "Triplet loss: 0.060388315469026566\n",
      "Triplet loss: 0.038978565484285355\n",
      "Triplet loss: 0.014181803911924362\n",
      "Triplet loss: 0.0053993938490748405\n",
      "Triplet loss: 0.0395398810505867\n",
      "Triplet loss: 0.007855324074625969\n",
      "Triplet loss: 0.007062859833240509\n",
      "Triplet loss: 0.02318195439875126\n",
      "Triplet loss: 0.02405184507369995\n",
      "Triplet loss: 0.014799972996115685\n",
      "Triplet loss: 0.008104369044303894\n",
      "Triplet loss: 0.002257324755191803\n",
      "Triplet loss: 0.017511408776044846\n",
      "Triplet loss: 0.007853294722735882\n",
      "Triplet loss: 0.003260047174990177\n",
      "Triplet loss: 0.005175010301172733\n",
      "Triplet loss: 0.0026377802714705467\n",
      "Triplet loss: 0.018542040139436722\n",
      "Triplet loss: 0.004663365893065929\n",
      "Triplet loss: 0.005123302340507507\n",
      "Triplet loss: 0.007669147104024887\n",
      "Triplet loss: 0.0032876422628760338\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0046106968075037\n",
      "Triplet loss: 0.010185758583247662\n",
      "Triplet loss: 0.002241457812488079\n",
      "Triplet loss: 0.003509749658405781\n",
      "Triplet loss: 0.000518222339451313\n",
      "Triplet loss: 0.0002717049792408943\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.002726130187511444\n",
      "Triplet loss: 0.005628894083201885\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.00023329537361860275\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0034358883276581764\n",
      "Triplet loss: 0.0007162345573306084\n",
      "Triplet loss: 0.0012616347521543503\n",
      "Triplet loss: 0.0051764827221632\n",
      "Triplet loss: 0.004687477834522724\n",
      "Triplet loss: 0.003239797428250313\n",
      "Triplet loss: 0.008037751540541649\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0005150921642780304\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0004992857575416565\n",
      "Triplet loss: 0.0005724858492612839\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0027725258842110634\n",
      "Triplet loss: 0.01215693261474371\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.003441781736910343\n",
      "Triplet loss: 0.0049280328676104546\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.005463502369821072\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0007084934040904045\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0032226024195551872\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.005270075052976608\n",
      "Triplet loss: 0.0011898120865225792\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0034892987459897995\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0007120268419384956\n",
      "Triplet loss: 0.004476887173950672\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.007080894894897938\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.005463852547109127\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0020378949120640755\n",
      "Triplet loss: 0.0\n",
      "Triplet loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "from model.twotowermodel import DocumentTower, QueryTower\n",
    "from data.dataset import QueryDocumentDataset\n",
    "from data.marco import get_training_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loss_function import triplet_loss_function\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "document_model = DocumentTower(embedding_dim=300, hidden_dim=128).to(device)\n",
    "query_model = QueryTower(embedding_dim=300, hidden_dim=128).to(device)\n",
    "\n",
    "optimizer = Adam(list(document_model.parameters()) + list(query_model.parameters()), lr=0.001)\n",
    "\n",
    "dataset_instance = QueryDocumentDataset(data=get_training_dataset(), embedding_model=model)\n",
    "dataloader = DataLoader(dataset_instance, batch_size=64, shuffle=False, collate_fn=custom_collate)  # You can adjust batch_size as needed\n",
    "\n",
    "\n",
    "counter = 0\n",
    "print('starting training')\n",
    "\n",
    "for query_emb, relevant_doc_emb, irrelevant_doc_emb in dataloader:\n",
    "    # Convert the tokens to embeddings\n",
    "    # Forward pass to get encodings for two tower\n",
    "\n",
    "    query_emb = query_emb.to(device)\n",
    "    relevant_doc_emb = relevant_doc_emb.to(device)\n",
    "    irrelevant_doc_emb = irrelevant_doc_emb.to(device)\n",
    "\n",
    "    relevant_doc_encoding, irrelevant_doc_encoding = document_model(relevant_doc_emb, irrelevant_doc_emb)\n",
    "    query_encoding = query_model(query_emb)\n",
    "\n",
    "    # Compute triplet loss\n",
    "    loss = triplet_loss_function(query_encoding, relevant_doc_encoding, irrelevant_doc_encoding, margin=0.3)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(f\"Triplet loss: {loss.item()}\")\n",
    "\n",
    "    if counter % 1000 == 0:\n",
    "        torch.save(document_model.state_dict(), 'document_model_state_dict.pth')\n",
    "        torch.save(query_model.state_dict(), 'query_model_state_dict.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
