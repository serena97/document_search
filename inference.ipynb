{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.twotowermodel import DocumentTower, QueryTower\n",
    "from data.dataset import QueryDocumentDataset\n",
    "import torch\n",
    "\n",
    "document_model = DocumentTower(embedding_dim=300, hidden_dim=512)\n",
    "query_model = QueryTower(embedding_dim=300, hidden_dim=512)\n",
    "\n",
    "document_model.load_state_dict(torch.load('document_model_state_dict1.pth'))\n",
    "query_model.load_state_dict(torch.load('query_model_state_dict1.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryTower(\n",
       "  (query_encoder): RNNEncoder(\n",
       "    (rnn): RNN(300, 512, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_model.eval()\n",
    "query_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import QueryDocumentDataset\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "model_path = \"./artifacts/word2vec-300.bin\"\n",
    "\n",
    "model = Word2Vec.load(model_path)\n",
    "dataset_instance = QueryDocumentDataset(data=[], embedding_model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "MAX_SEQ_LENGTH = 128  # Example maximum length\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "\n",
    "def get_docs_embedding(docs, batch_size=64):\n",
    "    all_doc_encodings = []\n",
    "\n",
    "    # Move your model to the appropriate device\n",
    "    document_model.to(device)\n",
    "\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        for doc in batch_docs:\n",
    "            tokenized_doc = simple_preprocess(doc)\n",
    "            doc_embeddings = torch.tensor([model.wv[word] for word in tokenized_doc if word in model.wv],dtype=torch.float32)\n",
    "\n",
    "            if len(doc_embeddings) > MAX_SEQ_LENGTH:\n",
    "                doc_embeddings = doc_embeddings[:MAX_SEQ_LENGTH]\n",
    "            else:\n",
    "                padding = MAX_SEQ_LENGTH - len(doc_embeddings)\n",
    "                pad_tensor = torch.zeros((padding, model.vector_size), dtype=torch.float32)\n",
    "                doc_embeddings = torch.cat((doc_embeddings, pad_tensor), dim=0)\n",
    "            batch_embeddings.append(doc_embeddings)\n",
    "\n",
    "        embeddings_tensor = torch.stack(batch_embeddings) if len(batch_embeddings) > 1 else batch_embeddings[0].unsqueeze(0)  # Ensure batch dimension\n",
    "        embeddings_tensor = embeddings_tensor.to(device)\n",
    "\n",
    "        \n",
    "        doc_encodings_batch = document_model.encode_single_doc(embeddings_tensor)\n",
    "        all_doc_encodings.extend(doc_encodings_batch.detach().cpu().numpy())\n",
    "    \n",
    "    final_embeddings = np.array(all_doc_encodings)\n",
    "    print(final_embeddings.shape)\n",
    "    return final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 676193\n",
      "(676193, 512)\n",
      "Number of docs_embedding: 676193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data.marco import get_all_documents\n",
    "\n",
    "documents = (get_all_documents())\n",
    "flattened_documents = [item for sublist in documents for item in sublist]\n",
    "print(f\"Number of documents: {len(flattened_documents)}\")\n",
    "\n",
    "docs_embedding = get_docs_embedding(flattened_documents)\n",
    "\n",
    "# Save to disk\n",
    "np.save(\"docs_embeddings_model1.npy\", docs_embedding)\n",
    "print(f\"Number of docs_embedding: {len(docs_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data.marco import get_all_documents\n",
    "# documents = (get_all_documents())\n",
    "# flattened_documents = [item for sublist in documents for item in sublist]\n",
    "\n",
    "flattened_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs_embedding: 676193\n",
      "dimension 512\n",
      "index\n",
      "Adding chunk i: 0\n",
      "Adding chunk i: 5000\n",
      "Adding chunk i: 10000\n",
      "Adding chunk i: 15000\n",
      "Adding chunk i: 20000\n",
      "Adding chunk i: 25000\n",
      "Adding chunk i: 30000\n",
      "Adding chunk i: 35000\n",
      "Adding chunk i: 40000\n",
      "Adding chunk i: 45000\n",
      "Adding chunk i: 50000\n",
      "Adding chunk i: 55000\n",
      "Adding chunk i: 60000\n",
      "Adding chunk i: 65000\n",
      "Adding chunk i: 70000\n",
      "Adding chunk i: 75000\n",
      "Adding chunk i: 80000\n",
      "Adding chunk i: 85000\n",
      "Adding chunk i: 90000\n",
      "Adding chunk i: 95000\n",
      "Adding chunk i: 100000\n",
      "Adding chunk i: 105000\n",
      "Adding chunk i: 110000\n",
      "Adding chunk i: 115000\n",
      "Adding chunk i: 120000\n",
      "Adding chunk i: 125000\n",
      "Adding chunk i: 130000\n",
      "Adding chunk i: 135000\n",
      "Adding chunk i: 140000\n",
      "Adding chunk i: 145000\n",
      "Adding chunk i: 150000\n",
      "Adding chunk i: 155000\n",
      "Adding chunk i: 160000\n",
      "Adding chunk i: 165000\n",
      "Adding chunk i: 170000\n",
      "Adding chunk i: 175000\n",
      "Adding chunk i: 180000\n",
      "Adding chunk i: 185000\n",
      "Adding chunk i: 190000\n",
      "Adding chunk i: 195000\n",
      "Adding chunk i: 200000\n",
      "Adding chunk i: 205000\n",
      "Adding chunk i: 210000\n",
      "Adding chunk i: 215000\n",
      "Adding chunk i: 220000\n",
      "Adding chunk i: 225000\n",
      "Adding chunk i: 230000\n",
      "Adding chunk i: 235000\n",
      "Adding chunk i: 240000\n",
      "Adding chunk i: 245000\n",
      "Adding chunk i: 250000\n",
      "Adding chunk i: 255000\n",
      "Adding chunk i: 260000\n",
      "Adding chunk i: 265000\n",
      "Adding chunk i: 270000\n",
      "Adding chunk i: 275000\n",
      "Adding chunk i: 280000\n",
      "Adding chunk i: 285000\n",
      "Adding chunk i: 290000\n",
      "Adding chunk i: 295000\n",
      "Adding chunk i: 300000\n",
      "Adding chunk i: 305000\n",
      "Adding chunk i: 310000\n",
      "Adding chunk i: 315000\n",
      "Adding chunk i: 320000\n",
      "Adding chunk i: 325000\n",
      "Adding chunk i: 330000\n",
      "Adding chunk i: 335000\n",
      "Adding chunk i: 340000\n",
      "Adding chunk i: 345000\n",
      "Adding chunk i: 350000\n",
      "Adding chunk i: 355000\n",
      "Adding chunk i: 360000\n",
      "Adding chunk i: 365000\n",
      "Adding chunk i: 370000\n",
      "Adding chunk i: 375000\n",
      "Adding chunk i: 380000\n",
      "Adding chunk i: 385000\n",
      "Adding chunk i: 390000\n",
      "Adding chunk i: 395000\n",
      "Adding chunk i: 400000\n",
      "Adding chunk i: 405000\n",
      "Adding chunk i: 410000\n",
      "Adding chunk i: 415000\n",
      "Adding chunk i: 420000\n",
      "Adding chunk i: 425000\n",
      "Adding chunk i: 430000\n",
      "Adding chunk i: 435000\n",
      "Adding chunk i: 440000\n",
      "Adding chunk i: 445000\n",
      "Adding chunk i: 450000\n",
      "Adding chunk i: 455000\n",
      "Adding chunk i: 460000\n",
      "Adding chunk i: 465000\n",
      "Adding chunk i: 470000\n",
      "Adding chunk i: 475000\n",
      "Adding chunk i: 480000\n",
      "Adding chunk i: 485000\n",
      "Adding chunk i: 490000\n",
      "Adding chunk i: 495000\n",
      "Adding chunk i: 500000\n",
      "Adding chunk i: 505000\n",
      "Adding chunk i: 510000\n",
      "Adding chunk i: 515000\n",
      "Adding chunk i: 520000\n",
      "Adding chunk i: 525000\n",
      "Adding chunk i: 530000\n",
      "Adding chunk i: 535000\n",
      "Adding chunk i: 540000\n",
      "Adding chunk i: 545000\n",
      "Adding chunk i: 550000\n",
      "Adding chunk i: 555000\n",
      "Adding chunk i: 560000\n",
      "Adding chunk i: 565000\n",
      "Adding chunk i: 570000\n",
      "Adding chunk i: 575000\n",
      "Adding chunk i: 580000\n",
      "Adding chunk i: 585000\n",
      "Adding chunk i: 590000\n",
      "Adding chunk i: 595000\n",
      "Adding chunk i: 600000\n",
      "Adding chunk i: 605000\n",
      "Adding chunk i: 610000\n",
      "Adding chunk i: 615000\n",
      "Adding chunk i: 620000\n",
      "Adding chunk i: 625000\n",
      "Adding chunk i: 630000\n",
      "Adding chunk i: 635000\n",
      "Adding chunk i: 640000\n",
      "Adding chunk i: 645000\n",
      "Adding chunk i: 650000\n",
      "Adding chunk i: 655000\n",
      "Adding chunk i: 660000\n",
      "Adding chunk i: 665000\n",
      "Adding chunk i: 670000\n",
      "Adding chunk i: 675000\n",
      "Index added\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "docs_embedding = np.load(\"docs_embeddings_model1.npy\", mmap_mode='r')\n",
    "\n",
    "print(f\"Number of docs_embedding: {len(docs_embedding)}\")\n",
    "\n",
    "\n",
    "dimension = docs_embedding.shape[1]\n",
    "print('dimension', dimension)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "print('index')\n",
    "\n",
    "chunk_size = 5000  # Adjust the chunk size based on your available system memory\n",
    "save_interval = 500  # Save the index every 500 chunks\n",
    "\n",
    "for i in range(0, len(docs_embedding), chunk_size):\n",
    "    print(\"Adding chunk i:\", i)\n",
    "    # Since docs_embedding is memory-mapped, slicing it does not load the entire array into memory\n",
    "    chunk = docs_embedding[i:i + chunk_size]\n",
    "    index.add(chunk)  # Add chunks incrementally to the FAISS index\n",
    "\n",
    "    if (i // chunk_size + 1) % save_interval == 0:\n",
    "        # Save the index to disk\n",
    "        faiss.write_index(index, f\"temp_index_{i // chunk_size + 1}.index\")\n",
    "        print(f\"Saved index at chunk {i // chunk_size + 1}\")\n",
    "\n",
    "faiss.write_index(index, f\"temp_index_test.index\")\n",
    "print('Index added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded\n",
      "torch.Size([512])\n",
      "Indices of nearest neighbors: [[493449 333101 519278 154609 464028]]\n",
      "Distances to nearest neighbors: [[899.4175  899.42236 899.4288  899.4336  899.4425 ]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "faiss.omp_set_num_threads(1)\n",
    "\n",
    "index = faiss.read_index(\"temp_index_test.index\", faiss.IO_FLAG_ONDISK_SAME_DIR)\n",
    "print(\"Index loaded\")\n",
    "# Search the index for the top k most similar documents\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "\n",
    "# unsqueeze it to add an extra dimension, making it a 2D tensor with shape (1, 128)\n",
    "query = \"what is a cat\"\n",
    "query_emb = dataset_instance.get_query_embedding(query) \n",
    "query_emb = query_model(query_emb)\n",
    "query_emb_2d = query_emb.unsqueeze(0).detach().numpy()\n",
    "print(query_emb.shape)\n",
    "\n",
    "D, I = index.search(query_emb_2d, k)  # D: distances, I: indices of the neighbors\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", I)\n",
    "print(\"Distances to nearest neighbors:\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geddes Automotive Onehunga Auckland replace shock absorbers for cars vans & trucks. Most shock absorbers can not be repaired, the shocks need to be replaced! Car suspension shocks can normally not have the gas refilled, whereas boot and bonnet stays can have the gas replenished. It is always best to replace a pair of shocks, the left hand front or rear shock and the right hand front or rear shock. By replacing both LH & RH shock the vehicle will handle the road conditions better than if you just replace the damaged or oil leaking shock absorber. The following list of costs to replace a car truck shock absorber is approximate only. It is just a cost guide. If you provide the Make, Model, Year, Chassis number, Registration number and which shock needs replacing we can give you an estimate and/or a quote.\n",
      "A pall, which recalls the white garments given in baptism, as well as the Resurrection of Christ at Easter, is the heavy (usually white-coloured) cloth that is draped over a coffin. The term pallbearer is used to signify someone who bears the coffin which the pall covers. Some traditions distinguish between the roles of pallbearer and casket bearer. The former is a ceremonial position, carrying a tip of the pall or a cord attached to it. The latter do the actual heavy lifting and carrying. A pallbearer in the USA will carry a casket by the handles, and at around waist height. In the United Kingdom, Canada, and Ireland, the casket is carried on the shoulders, and the handles are for the most part decorative. All lifting should be done from underneath the casket.\n",
      "A MCHC test is a test that is carried out to test a person for anemia. The MCHC in a MCHC test stands for Mean Corpuscular Hemoglobin Concentration. MCHC is the calculation of the average hemoglobin inside a red blood cell. A MCHC test can be performed along with a MCV test (Mean Corpuscular Volume). Both levels are used to test people for anemia. The MCHC test is also known as the MCH blood test which tests the levels of hemoglobin in the blood. The MCHC test can be ordered as part of a complete blood count (CBC) test. A low MCHC count may indicate anemia but several other factors will have to be taken into consideration before coming to this conclusion. MCHC is measured in grams per deciliter. Normal readings for MCHC are 31 grams per deciliter to 35 grams per deciliter. A MCHC blood test may be ordered when a person is showing signs of fatigue or weakness, when there is an infection, is bleeding or bruising easily or when there is an inflammation\n",
      "Gujarati Typing (Type in Gujarati). Its very easy and simple to type in Gujarati (Gujarati Typing) using English. Just type the text in English in the given box and press space, it will convert the text in Gujarati script. Click on a word to see more options. To switch between Gujarati and English use ctrl + g. Now copy the text and use it anywhere on emails, chat, facebook, twitter or any website. Type in English and Press Space to Convert in Gujarati jtemplate.ru-free extensions Joomla. Convert These Unicode Font into Saumil Gujarati font. Type Gujarati is simpler with English to Gujarati Typing software, More the software provides Suggestion while typeing in english so you can choose the right word you want to type and also saves your valuable time in typing.\n",
      "Amplitude modulation basics. When an amplitude modulated signal is created, the amplitude of the signal is varied in line with the variations in intensity of the sound wave. In this way the overall amplitude or envelope of the carrier is modulated to carry the audio signal. Here the envelope of the carrier can be seen to change in line with the modulating signal. Amplitude Modulation. Amplitude modulation, AM is the most straightforward way of modulating a signal. Demodulation, or the process where the radio frequency signal is converted into an audio frequency signal is also very simple. An amplitude modulation signal only requires a simple diode detector circuit. There are a number of ways in which a carrier can be modulated to carry a signal-often an audio signal and the most obvious way is to vary its amplitude. Amplitude Modulation has been in use since the very earliest days of radio technology.\n"
     ]
    }
   ],
   "source": [
    "top5_documents = I.flatten()\n",
    "# docs_embedding = np.load(\"docs_embeddings_model1.npy\", mmap_mode='r')\n",
    "\n",
    "\n",
    "for idx in top5_documents:\n",
    "    print(flattened_documents[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_documents[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackernews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
