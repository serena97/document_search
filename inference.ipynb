{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.twotowermodel import DocumentTower, QueryTower\n",
    "from data.dataset import QueryDocumentDataset\n",
    "import torch\n",
    "\n",
    "document_model = DocumentTower(embedding_dim=300, hidden_dim=128)\n",
    "query_model = QueryTower(embedding_dim=300, hidden_dim=128)\n",
    "\n",
    "document_model.load_state_dict(torch.load('document_model_state_dict.pth'))\n",
    "query_model.load_state_dict(torch.load('query_model_state_dict.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryTower(\n",
       "  (query_encoder): RNNEncoder(\n",
       "    (rnn): GRU(300, 128, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_model.eval()\n",
    "query_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import QueryDocumentDataset\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "model_path = \"./artifacts/word2vec-300.bin\"\n",
    "\n",
    "model = Word2Vec.load(model_path)\n",
    "dataset_instance = QueryDocumentDataset(data=[], embedding_model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "MAX_SEQ_LENGTH = 128  # Example maximum length\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "\n",
    "def get_docs_embedding(docs, batch_size=64):\n",
    "    all_doc_encodings = []\n",
    "\n",
    "    # Move your model to the appropriate device\n",
    "    document_model.to(device)\n",
    "\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        for doc in batch_docs:\n",
    "            tokenized_doc = simple_preprocess(doc)\n",
    "            doc_embeddings = torch.tensor([model.wv[word] for word in tokenized_doc if word in model.wv],dtype=torch.float32)\n",
    "\n",
    "            if len(doc_embeddings) > MAX_SEQ_LENGTH:\n",
    "                doc_embeddings = doc_embeddings[:MAX_SEQ_LENGTH]\n",
    "            else:\n",
    "                padding = MAX_SEQ_LENGTH - len(doc_embeddings)\n",
    "                pad_tensor = torch.zeros((padding, model.vector_size), dtype=torch.float32)\n",
    "                doc_embeddings = torch.cat((doc_embeddings, pad_tensor), dim=0)\n",
    "            batch_embeddings.append(doc_embeddings)\n",
    "\n",
    "        embeddings_tensor = torch.stack(batch_embeddings) if len(batch_embeddings) > 1 else batch_embeddings[0].unsqueeze(0)  # Ensure batch dimension\n",
    "        embeddings_tensor = embeddings_tensor.to(device)\n",
    "\n",
    "        \n",
    "        doc_encodings_batch = document_model.encode_single_doc(embeddings_tensor)\n",
    "        all_doc_encodings.extend(doc_encodings_batch.detach().cpu().numpy())\n",
    "    \n",
    "    final_embeddings = np.array(all_doc_encodings)\n",
    "    print(final_embeddings.shape)\n",
    "    return final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 676193\n",
      "(676193, 128)\n",
      "Number of docs_embedding: 676193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data.marco import get_all_documents\n",
    "\n",
    "documents = (get_all_documents())\n",
    "flattened_documents = [item for sublist in documents for item in sublist]\n",
    "print(f\"Number of documents: {len(flattened_documents)}\")\n",
    "\n",
    "docs_embedding = get_docs_embedding(flattened_documents)\n",
    "\n",
    "# Save to disk\n",
    "np.save(\"docs_embeddings_model.npy\", docs_embedding)\n",
    "print(f\"Number of docs_embedding: {len(docs_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.marco import get_all_documents\n",
    "documents = (get_all_documents())\n",
    "flattened_documents = [item for sublist in documents for item in sublist]\n",
    "\n",
    "flattened_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs_embedding: 676193\n",
      "dimension 128\n",
      "index\n",
      "Adding chunk i: 0\n",
      "Adding chunk i: 5000\n",
      "Adding chunk i: 10000\n",
      "Adding chunk i: 15000\n",
      "Adding chunk i: 20000\n",
      "Adding chunk i: 25000\n",
      "Adding chunk i: 30000\n",
      "Adding chunk i: 35000\n",
      "Adding chunk i: 40000\n",
      "Adding chunk i: 45000\n",
      "Adding chunk i: 50000\n",
      "Adding chunk i: 55000\n",
      "Adding chunk i: 60000\n",
      "Adding chunk i: 65000\n",
      "Adding chunk i: 70000\n",
      "Adding chunk i: 75000\n",
      "Adding chunk i: 80000\n",
      "Adding chunk i: 85000\n",
      "Adding chunk i: 90000\n",
      "Adding chunk i: 95000\n",
      "Adding chunk i: 100000\n",
      "Adding chunk i: 105000\n",
      "Adding chunk i: 110000\n",
      "Adding chunk i: 115000\n",
      "Adding chunk i: 120000\n",
      "Adding chunk i: 125000\n",
      "Adding chunk i: 130000\n",
      "Adding chunk i: 135000\n",
      "Adding chunk i: 140000\n",
      "Adding chunk i: 145000\n",
      "Adding chunk i: 150000\n",
      "Adding chunk i: 155000\n",
      "Adding chunk i: 160000\n",
      "Adding chunk i: 165000\n",
      "Adding chunk i: 170000\n",
      "Adding chunk i: 175000\n",
      "Adding chunk i: 180000\n",
      "Adding chunk i: 185000\n",
      "Adding chunk i: 190000\n",
      "Adding chunk i: 195000\n",
      "Adding chunk i: 200000\n",
      "Adding chunk i: 205000\n",
      "Adding chunk i: 210000\n",
      "Adding chunk i: 215000\n",
      "Adding chunk i: 220000\n",
      "Adding chunk i: 225000\n",
      "Adding chunk i: 230000\n",
      "Adding chunk i: 235000\n",
      "Adding chunk i: 240000\n",
      "Adding chunk i: 245000\n",
      "Adding chunk i: 250000\n",
      "Adding chunk i: 255000\n",
      "Adding chunk i: 260000\n",
      "Adding chunk i: 265000\n",
      "Adding chunk i: 270000\n",
      "Adding chunk i: 275000\n",
      "Adding chunk i: 280000\n",
      "Adding chunk i: 285000\n",
      "Adding chunk i: 290000\n",
      "Adding chunk i: 295000\n",
      "Adding chunk i: 300000\n",
      "Adding chunk i: 305000\n",
      "Adding chunk i: 310000\n",
      "Adding chunk i: 315000\n",
      "Adding chunk i: 320000\n",
      "Adding chunk i: 325000\n",
      "Adding chunk i: 330000\n",
      "Adding chunk i: 335000\n",
      "Adding chunk i: 340000\n",
      "Adding chunk i: 345000\n",
      "Adding chunk i: 350000\n",
      "Adding chunk i: 355000\n",
      "Adding chunk i: 360000\n",
      "Adding chunk i: 365000\n",
      "Adding chunk i: 370000\n",
      "Adding chunk i: 375000\n",
      "Adding chunk i: 380000\n",
      "Adding chunk i: 385000\n",
      "Adding chunk i: 390000\n",
      "Adding chunk i: 395000\n",
      "Adding chunk i: 400000\n",
      "Adding chunk i: 405000\n",
      "Adding chunk i: 410000\n",
      "Adding chunk i: 415000\n",
      "Adding chunk i: 420000\n",
      "Adding chunk i: 425000\n",
      "Adding chunk i: 430000\n",
      "Adding chunk i: 435000\n",
      "Adding chunk i: 440000\n",
      "Adding chunk i: 445000\n",
      "Adding chunk i: 450000\n",
      "Adding chunk i: 455000\n",
      "Adding chunk i: 460000\n",
      "Adding chunk i: 465000\n",
      "Adding chunk i: 470000\n",
      "Adding chunk i: 475000\n",
      "Adding chunk i: 480000\n",
      "Adding chunk i: 485000\n",
      "Adding chunk i: 490000\n",
      "Adding chunk i: 495000\n",
      "Adding chunk i: 500000\n",
      "Adding chunk i: 505000\n",
      "Adding chunk i: 510000\n",
      "Adding chunk i: 515000\n",
      "Adding chunk i: 520000\n",
      "Adding chunk i: 525000\n",
      "Adding chunk i: 530000\n",
      "Adding chunk i: 535000\n",
      "Adding chunk i: 540000\n",
      "Adding chunk i: 545000\n",
      "Adding chunk i: 550000\n",
      "Adding chunk i: 555000\n",
      "Adding chunk i: 560000\n",
      "Adding chunk i: 565000\n",
      "Adding chunk i: 570000\n",
      "Adding chunk i: 575000\n",
      "Adding chunk i: 580000\n",
      "Adding chunk i: 585000\n",
      "Adding chunk i: 590000\n",
      "Adding chunk i: 595000\n",
      "Adding chunk i: 600000\n",
      "Adding chunk i: 605000\n",
      "Adding chunk i: 610000\n",
      "Adding chunk i: 615000\n",
      "Adding chunk i: 620000\n",
      "Adding chunk i: 625000\n",
      "Adding chunk i: 630000\n",
      "Adding chunk i: 635000\n",
      "Adding chunk i: 640000\n",
      "Adding chunk i: 645000\n",
      "Adding chunk i: 650000\n",
      "Adding chunk i: 655000\n",
      "Adding chunk i: 660000\n",
      "Adding chunk i: 665000\n",
      "Adding chunk i: 670000\n",
      "Adding chunk i: 675000\n",
      "Index added\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "docs_embedding = np.load(\"docs_embeddings_model.npy\", mmap_mode='r')\n",
    "\n",
    "print(f\"Number of docs_embedding: {len(docs_embedding)}\")\n",
    "\n",
    "\n",
    "dimension = docs_embedding.shape[1]\n",
    "print('dimension', dimension)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "print('index')\n",
    "\n",
    "chunk_size = 5000  # Adjust the chunk size based on your available system memory\n",
    "save_interval = 500  # Save the index every 500 chunks\n",
    "\n",
    "for i in range(0, len(docs_embedding), chunk_size):\n",
    "    print(\"Adding chunk i:\", i)\n",
    "    # Since docs_embedding is memory-mapped, slicing it does not load the entire array into memory\n",
    "    chunk = docs_embedding[i:i + chunk_size]\n",
    "    index.add(chunk)  # Add chunks incrementally to the FAISS index\n",
    "\n",
    "    if (i // chunk_size + 1) % save_interval == 0:\n",
    "        # Save the index to disk\n",
    "        faiss.write_index(index, f\"temp_index_{i // chunk_size + 1}.index\")\n",
    "        print(f\"Saved index at chunk {i // chunk_size + 1}\")\n",
    "\n",
    "faiss.write_index(index, f\"temp_index_test.index\")\n",
    "print('Index added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded\n",
      "torch.Size([128])\n",
      "Indices of nearest neighbors: [[108595 540659  45617 353242 466727]]\n",
      "Distances to nearest neighbors: [[10.550879 10.625858 10.625963 10.626002 10.626014]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "faiss.omp_set_num_threads(1)\n",
    "\n",
    "index = faiss.read_index(\"temp_index_test.index\", faiss.IO_FLAG_ONDISK_SAME_DIR)\n",
    "print(\"Index loaded\")\n",
    "# Search the index for the top k most similar documents\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "\n",
    "# unsqueeze it to add an extra dimension, making it a 2D tensor with shape (1, 128)\n",
    "query = \"how long is german measles contagious?\"\n",
    "query_emb = dataset_instance.get_query_embedding(query) \n",
    "query_emb = query_model(query_emb)\n",
    "query_emb_2d = query_emb.unsqueeze(0).detach().numpy()\n",
    "print(query_emb.shape)\n",
    "\n",
    "D, I = index.search(query_emb_2d, k)  # D: distances, I: indices of the neighbors\n",
    "\n",
    "print(\"Indices of nearest neighbors:\", I)\n",
    "print(\"Distances to nearest neighbors:\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenders are in charge of setting their own guidelines in reference to how long a pre-approval is valid. Some lenders will set a guideline of 45 days, while others will go as long as 90 days. However, should a pre-approval letter expire because the borrower hasn't found the right house, he can obtain a new pre-approval letter once the lender has re-verified his income, debt and credit situation. Mortgage pre-approvals are letters issued by a bank after a consumer's income, debt and credit history have been reviewed. These letters provide a statement showing the total amount of a loan that a consumer is approved to obtain in a home purchase.\n",
      "FHA Mortgage Job Requirements. First, the lender must look at the last two full years of employment. For this reason it would be smart to have tax returns available for the last three years. Second, HUD not only wants to know that you now have a job, it also wants some sense that FHA borrowers will have a job in the future. \n",
      "FHA Upfront Mortgage Insurance Premiums. The FHA's current upfront mortgage insurance premium (UFMIP) is 1.75 percent of your loan size. For example, if you use an FHA-backed mortgage for a purchase mortgage and your loan size is $300,000, then your Upfront MIP will be 1.75 percent of $300,000, or $5,250. Upfront MIP is not paid as cash.\n",
      "FHA loans require low down payments — typically only 3.5 percent — and low closing costs, many of which can be included in the loan. The FHA also offers loans that allow you to purchase a home in need of repairs and to roll the cost of the fixes into the primary mortgage loan.\n",
      "UberX is the “ride sharing” service that matches riders with independent drivers who use their own personal vehicle. UberX drivers are not professional livery drivers and a commercial license is not required, although some UberX drivers might have one. UberX drivers must be 21 or older with a clean driving record.\n"
     ]
    }
   ],
   "source": [
    "top5_documents = I.flatten()\n",
    "docs_embedding = np.load(\"docs_embeddings_model.npy\", mmap_mode='r')\n",
    "\n",
    "\n",
    "for idx in top5_documents:\n",
    "    print(flattened_documents[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_documents[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackernews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
